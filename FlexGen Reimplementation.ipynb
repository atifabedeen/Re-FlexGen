{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:43:21.308435Z","iopub.status.busy":"2024-04-22T16:43:21.307782Z","iopub.status.idle":"2024-04-22T16:43:27.566040Z","shell.execute_reply":"2024-04-22T16:43:27.565166Z","shell.execute_reply.started":"2024-04-22T16:43:21.308394Z"},"id":"MlKrSUXNoxLx","trusted":true},"outputs":[],"source":["import time\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import random\n","from torch import optim\n","import matplotlib.pyplot as plt\n","from typing import List\n","import torch.optim as optim\n","from torch.autograd import profiler\n","import gc\n","from torch.utils.data import DataLoader\n","\n","from transformers import AutoTokenizer\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:43:27.567882Z","iopub.status.busy":"2024-04-22T16:43:27.567509Z","iopub.status.idle":"2024-04-22T16:43:27.577823Z","shell.execute_reply":"2024-04-22T16:43:27.576947Z","shell.execute_reply.started":"2024-04-22T16:43:27.567857Z"},"id":"0WbsmndBpqix","trusted":true},"outputs":[],"source":["\n","class InputEmbedding(nn.Module):\n","    def __init__(self, d_model, vocab_size):\n","        super(InputEmbedding, self).__init__()\n","        self.input_embed = nn.Embedding(vocab_size, d_model)\n","\n","    def forward(self, x):\n","        x = self.input_embed(x)\n","        return x\n","\n","\n","class OutputEmbedding(nn.Module):\n","    def __init__(self, d_model, vocab_size):\n","        super(OutputEmbedding, self).__init__()\n","        self.output_embed = nn.Linear(d_model, vocab_size)\n","\n","\n","    def forward(self, x):\n","        x = self.output_embed(x)\n","        return x\n","\n","class MLP(nn.Module):\n","    def __init__(self, d_model):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.linear1 = nn.Linear(d_model, d_model)\n","        self.linear2 = nn.Linear(d_model, d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","\n","\n","    def forward(self, x):\n","        x = self.norm1(x)\n","        x = nn.ReLU()(self.linear1(x))\n","        x = self.linear2(x)\n","        x = self.norm2(x)\n","        return x\n","\n","    def initialize_weights(self):\n","        self.linear1.weight.fill_(1)\n","        self.linear2.weight.fill_(1)\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:43:27.579321Z","iopub.status.busy":"2024-04-22T16:43:27.578998Z","iopub.status.idle":"2024-04-22T16:43:27.591571Z","shell.execute_reply":"2024-04-22T16:43:27.590737Z","shell.execute_reply.started":"2024-04-22T16:43:27.579298Z"},"id":"WeXzoHEjp1OM","trusted":true},"outputs":[],"source":["\n","class TransformerLayer(nn.Module):\n","    def __init__(self, d_model, d_internal, n_heads):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.d_internal = d_internal\n","        self.q_weight = nn.Linear(d_model, d_internal)\n","        self.k_weight = nn.Linear(d_model, d_internal)\n","        self.v_weight = nn.Linear(d_model, d_model)\n","        self.mlp = MLP(d_model)\n","        self.mha = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n","\n","    def initialize_weights(self):\n","        with torch.no_grad():\n","            ### Here we initiaize to 1, but realistically the pre-trainied models weights should be read\n","            self.q_weight.weight.fill_(1)\n","            self.k_weight.weight.fill_(1)\n","            self.v_weight.weight.fill_(1)\n","            self.mlp.initialize_weights()\n","\n","        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        # self.q_weight.to(device)\n","        # self.k_weight.to(device)\n","        # self.v_weight.to(device)\n","        # self.mha.to(device)\n","\n","        #### Remeber to send this on the GPU\n","        layer_weights = {\n","            \"weight_q\": self.q_weight,\n","            \"weight_k\": self.k_weight,\n","            \"weight_v\": self.v_weight,\n","            \"mha\" : self.mha\n","        }\n","        return layer_weights\n","\n","\n","    def forward(self, k, i, hidden, weights, cache):\n","        if i == 1:\n","            input_vecs = data[k]\n","        else:\n","            input_vecs = hidden\n","        ## K, Q, V calculations\n","        q = weights[\"q_weight\"](input_vecs)\n","        k = weights[\"k_weight\"](input_vecs)\n","        v = weights[\"v_weight\"](input_vecs)\n","\n","        # attention_map = torch.matmul(q, torch.transpose(k, 0, 1)) ## Make sure transpose return the correct dim\n","        # sm_normalized_attention_map = nn.Softmax()(attention_map / (self.d_internal ** 0.5))\n","        # h = torch.matmul(sm_normalized_attention_map, v)\n","        h = self.mha(q, k, v)\n","\n","        ## residual connection\n","        x_residual = h + input_vecs\n","\n","        ## Feed Forward layers``\n","        x = self.linear1(x_residual)\n","        x = nn.ReLU()(x)\n","        x = self.linear2(x)\n","\n","        ## Add residual connection\n","        x = x + x_residual\n","\n","        hidden = x\n","        return hidden, k, v\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:43:27.594275Z","iopub.status.busy":"2024-04-22T16:43:27.593855Z","iopub.status.idle":"2024-04-22T16:43:27.604220Z","shell.execute_reply":"2024-04-22T16:43:27.603318Z","shell.execute_reply.started":"2024-04-22T16:43:27.594252Z"},"id":"XJC0dYWXp3VQ","trusted":true},"outputs":[],"source":["\n","\n","class Transformer(nn.Module):\n","    def __init__(self, vocab_size, d_model, d_internal, num_classes, num_layers, gpu_batch_size, num_gpu_batches, n_heads):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.d_model = d_model\n","        self.d_internal = d_internal\n","        self.num_classes = num_classes\n","        self.num_layers = num_layers\n","        self.input_dim = d_model\n","\n","        self.attention_maps = []\n","        self.input_embed = InputEmbedding(self.d_model, self.vocab_size)\n","        self.transformer_layers = nn.ModuleList([TransformerLayer(self.d_model, self.d_internal, n_heads) for i in range(self.num_layers)])\n","        self.out_linear = OutputEmbedding(self.d_model, self.vocab_size)\n","\n","\n","\n","        # raise Exception(\"Implement me\")\n","\n","    def forward(self, indices):\n","        self.attention_maps = []\n","        x = self.input_embed(indices)\n","        for transformer_layer in self.transformer_layers:\n","            x, attention_map = transformer_layer(x)\n","            self.attention_maps.append(attention_map)\n","        x = self.linear(x)\n","        x = nn.Softmax()(x)\n","\n","        return x, self.attention_maps\n","\n","    def load_all_weights(self):\n","        all_weights = []\n","        for j in range(self.num_layers):\n","            all_weights.append(self.transformer_layers[j].initialize_weights())\n","            #print(f\"layer {j} initialized\")\n","        return all_weights\n","\n","# class FlexGen:\n","#     def __init__(self):\n","#         self.weight_home = []\n","#         self.prompt_len = 10\n","#         self.num_prompts = 2\n","\n","#     #copyied\n","#     def get_test_inputs(self, prompt_len, num_prompts, tokenizer):\n","#         prompts = [\"Paris is the capital city of\"]\n","#         input_ids = tokenizer(prompts, padding=\"max_length\",\n","#                             max_length=prompt_len).input_ids\n","#         return (input_ids[0],) * num_prompts\n","\n","#     ## Return something like (ids)*num_prompts\n","#     def get_test_inputs(self, tokenizer):\n","#         test_inputs = self.get_test_inputs(10, 2, tokenizer)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:43:27.605677Z","iopub.status.busy":"2024-04-22T16:43:27.605370Z","iopub.status.idle":"2024-04-22T16:43:27.617608Z","shell.execute_reply":"2024-04-22T16:43:27.616818Z","shell.execute_reply.started":"2024-04-22T16:43:27.605650Z"},"id":"LDOWGVuHVWSf","trusted":true},"outputs":[],"source":["def calc_KV_memory():\n","    max_seq_len = opt_config[\"max_seq_len\"]\n","    num_hidden_layers = opt_config[\"num_hidden_layers\"]\n","    num_gpu_batches = opt_config[\"num_gpu_batches\"]\n","    gpu_batch_size = opt_config[\"gpu_batch_size\"]\n","    input_dim = opt_config[\"input_dim\"]\n","\n","    # Calculate the maximum size of the KV cache\n","    max_kv_cache_size = num_hidden_layers * num_gpu_batches * gpu_batch_size * max_seq_len * input_dim\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":301,"referenced_widgets":["2698d060403345148b04bf63f193996e","f6ac6f9411014114809c06433921610a","8d9ae42aadb54690b974ec2cf6f74780","4ea04fbfd14d4bc0805f7a0b362a9671","eeb478610bc74e2f91b8461475e8200c","f11ffeb0db6d46d0a15f361d66864ef7","20e696a19a5d471ebdcc436ff8dbde34","5a08c7f346ab4bd9bb18e6a9cf793728","fcb370e58140494c801dcf517304491a","d3edd7e5aa9a4a8eb4077fa801a47971","a242facd21144b599656052e55cf62f7","3674a894012348909aea0d93c2efbec9","d9c30ff46e2e44d8a8cbef4e6e0f1bfc","d87f464484814cb2ae55aa64a74e2ec8","9d095d3e7a2c44069835cfb8b859836d","7afaa90e89414e2dad7ac07e911da8bf","9490adf32e9140a490b2fb48fa93a92e","084efcd4f2024c759dee11ecd94c69c0","59db37a6d3fe4900b1cad1f793dbac65","b5f90985c5f3427a83b32635d32c56d5","e18b86c903d541788ec2d7b3c0f91225","9135fd59b0814a1881bcf8985d3aa92b","c53ae763042a4b10b835cd21274f1589","9bad0992479b4212a2e04c507ef12f30","f1e7479089f846cf81c0f691ef6d1b9d","14d090c5d5b9426ebeba54371151619f","1e3f85e1ceff4da8bc39e683b5c75b2d","53bdc05005ea4bd6ac4dd7940a75dfaa","7bfb21a26bdf48cdba25b6018cfac792","dfdbe51a6f0e455b91c3c194dd15f064","2cb896dda4134ea4b07775a2df82e156","a51c974bef834123894086fbe9e02a32","a1dfc8865e044854a5d21f19525889e0","73f3d5ffa198476abe75e5d500e041ac","89448e748c1a49e0a158b86ebd064072","d6300a4882f64fe186aa69c822c886ee","a98a5e8ef09d4474873962de3158087c","367bd0c3cf2a48fa8006b14b3c00284a","0e4bfd2004244427969d6835aaf1da67","c8423a0b6d414315aa9714ea6416be53","d52880be825c46fea09ad8e1ee35f694","84517602469749a083990fa5fc3bcc8d","333a669d9cb545328eda001262eb042a","fad66a89c41e48bb80262cc3f002dc2b","75336ac6fb9045fca144a0a1d858b9e1","b550bb703a0b412ab564a42afb18fea5","38063cc56d96464c8887aeb7c619f16a","891232c8116a4c65ae28454cc57ddab5","941a33dd456f45298a0753c9d4930cda","79b250af1835464bbe12493a44423764","43be10eb97e04c98a6d99bbd3f5d2b63","167a39eb1956451dbf95c35bf7cf06ea","d134ff4e50db4521a18b66e167cefcf9","345c613d7363438084d81067d9a18306","1f3b3815c624467898f51bfdac858746"]},"execution":{"iopub.execute_input":"2024-04-22T16:43:27.619095Z","iopub.status.busy":"2024-04-22T16:43:27.618790Z"},"id":"2_kD8NeMp6EB","outputId":"05e50078-1862-41cc-c4d8-613054dc0b3b","trusted":true},"outputs":[],"source":["def main():\n","\n","    GPU = torch.device('cuda:0')\n","    CPU = torch.device('cpu')\n","\n","    opt_config = {\n","            \"name\":'opt-1.3',\n","            \"max_seq_len\": 24,\n","            \"num_hidden_layers\":24,\n","            \"n_head\":32,\n","            \"hidden_size\": 2048, #2048\n","            \"input_dim\": 2048, #2048\n","            \"ffn_embed_dim\":2048 * 4,\n","            \"pad\":  1,\n","            \"activation_fn\" : 'relu',\n","            \"vocab_size\":  50272,\n","            \"layer_norm_eps\":  0.00001,\n","            \"pad_token_id\":  1,\n","            \"dtype\": np.float16,\n","            \"num_gpu_batches\" : 1,\n","            \"gpu_batch_size\" : 16, #32\n","            \"prompt_len\": 512, #512\n","            \"file_size\" : 24\n","        }\n","\n","    transformer = Transformer(\n","        opt_config[\"vocab_size\"],\n","        opt_config[\"input_dim\"],\n","        opt_config[\"hidden_size\"],\n","        opt_config[\"vocab_size\"],\n","        opt_config[\"num_hidden_layers\"],\n","        opt_config[\"gpu_batch_size\"],\n","        opt_config[\"num_gpu_batches\"],\n","        opt_config[\"n_head\"]\n","        ).eval()\n","    transformer.to(GPU)\n","    \n","#     transformer = nn.DataParallel(transformer)\n","\n","    assert transformer.input_embed.input_embed.weight.device == GPU\n","    assert transformer.out_linear.output_embed.weight.device == GPU\n","    assert transformer.transformer_layers[0].q_weight.weight.device == GPU\n","\n","    all_weights = transformer.load_all_weights()\n","    num_prompts = opt_config[\"num_gpu_batches\"] * opt_config[\"gpu_batch_size\"]\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-30b\", padding_side=\"left\")\n","\n","\n","    def get_test_inputs(prompt_len, num_prompts, tokenizer):\n","        prompts = [\"Paris is the capital city of\"]\n","        input_ids = tokenizer(prompts, padding=\"max_length\",\n","                            max_length=prompt_len).input_ids\n","        return (input_ids[0],) * num_prompts\n","\n","    input = get_test_inputs(opt_config[\"prompt_len\"], num_prompts, tokenizer)\n","    input = torch.tensor(input)\n","    batches = torch.chunk(input, chunks=opt_config[\"num_gpu_batches\"])\n","    batches = [batch.to(GPU) for batch in batches]\n","    assert batches[0].device == GPU\n","    \"\"\"\n","\n","    Prefill\n","\n","InputEMB\n","\n","For layer 1 : j\n","\tLoad layer j in CPU\n","\tFor Batch 1 - k\n","\t\tif not layer 1\n","\t\t\tget activations of layer j-1 for batch k from GPU\n","\t\t\tx = Activation\n","\t\tElse:\n","\t\t\tx = Output from InputEmb\n","\n","\t\t- K = W_k*x, Q = W_q*x, V = W_v*x\n","\t\t- Attention Score -> Softmax(K.Q).V\n","\t\t- Send Attention Score to GPU\n","\t\t- Append the KV values to kv_home[j][k]\n","\t\t- Use attention score on GPU to calculate activation on GPU\n","\t\t\t- Overwrite the Activation[k]\n","\n","\n","OutputEmb\n","--------------\n","\n","\n","Decode (here i > 0)\n","\n","InputEmb\n","\n","For layer 1 : j\n","\tLoad layer j in CPU\n","\tFor Bath 1 - k\n","\tif not layer 1\n","\t\t\tget activations of layer j-1 for batch k from GPU\n","\t\t\tx = Activation\n","\tElse:\n","\t\t\tx = Output from InputEmb\n","\n","\told_k , old_v = Retrive kv_home[j][k]\n","\n","\t- new_k = W_k*x, Q = W_q*x, new_v = W_v*x\n","\t- Concat (old_k, new_k) + (old_v, new_v) to get new K,V\n","\n","\t- Attention Score -> Softmax(K.Q).V\n","\t- Send Attention Score to GPU\n","\t- Replace with the Concatted the KV values to kv_home[j][k]\n","\t- Use attention score on GPU to calculate activation on GPU\n","\t\t- Overwrite the Activation[k]\n","OutputEmbd\n","---------------------------------------------------------------------------------------\n","\n","\n","activations / Model should be on the GPU -> CPU -> Disk\n","kv cache on the CPU -> Disk\n","\"\"\"\n","\n","    act_home = torch.empty(opt_config[\"num_gpu_batches\"],opt_config[\"gpu_batch_size\"], opt_config[\"prompt_len\"], opt_config[\"input_dim\"]).to(GPU)\n","    #k_home = torch.empty(opt_config[\"num_hidden_layers\"], opt_config[\"num_gpu_batches\"], opt_config[\"gpu_batch_size\"], opt_config[\"prompt_len\"], opt_config[\"input_dim\"]).to(CPU)\n","    #v_home = torch.empty(opt_config[\"num_hidden_layers\"], opt_config[\"num_gpu_batches\"], opt_config[\"gpu_batch_size\"], opt_config[\"prompt_len\"], opt_config[\"input_dim\"]).to(CPU)\n","    k_home = torch.empty(opt_config[\"file_size\"], opt_config[\"num_gpu_batches\"], opt_config[\"gpu_batch_size\"], opt_config[\"prompt_len\"], opt_config[\"input_dim\"]).to(CPU)\n","    v_home = torch.empty(opt_config[\"file_size\"], opt_config[\"num_gpu_batches\"], opt_config[\"gpu_batch_size\"], opt_config[\"prompt_len\"], opt_config[\"input_dim\"]).to(CPU)\n","#     act_home = torch.empty(opt_config[\"gpu_batch_size\"], opt_config[\"prompt_len\"], opt_config[\"input_dim\"]).to(GPU)\n","    assert act_home.device == GPU\n","    assert k_home.device == CPU\n","    assert v_home.device == CPU\n","\n","\n","    def move_weights(layer_weights, device):\n","        for k, v in layer_weights.items():\n","            # assert v.weight.device == GPU\n","            if k == \"weight_l1\" or k == \"weight_l2\":\n","                continue\n","            v.to(device)\n","\n","            if k != 'mha':\n","                assert v.weight.device == device\n","            else:\n","                # assert v.device == CPU\n","                parameters_device = next(v.parameters()).device\n","                assert parameters_device == device\n","\n","    prefilling_start_time = time.time()\n","    remainder = opt_config[\"file_size\"]-1\n","    modder = opt_config[\"file_size\"]\n","    #### PREFILL\n","    print(\"Prefill\")\n","    with torch.no_grad():\n","        for j in range(opt_config[\"num_hidden_layers\"]):\n","            #with profiler.profile(use_cuda=True) as prof:\n","            print(j)\n","            layer_weights = all_weights[j]\n","            move_weights(layer_weights, CPU)\n","\n","\n","            for k in range(opt_config[\"num_gpu_batches\"]):\n","                if j != 0:\n","    #                 file_name = f\"a_{k}.pt\"\n","    #                 x = torch_load(file_name)\n","                      x = act_home[k]\n","                else:\n","                    x = transformer.input_embed(batches[k])#.pin_memory().to(GPU, non_blocking=True))\n","                    #print(f\"x = {x.shape}\")\n","                #print(j)\n","                x = x.to(CPU)\n","                K = layer_weights[\"weight_k\"](x)\n","                Q = layer_weights[\"weight_q\"](x)\n","                V = layer_weights[\"weight_v\"](x)\n","\n","                atten_map, _ = transformer.transformer_layers[j].mha(Q,K,V, need_weights=False)\n","                gc.collect()\n","                k_home[j%modder][k] = K\n","                v_home[j%modder][k] = V\n","\n","                K,Q,V = None, None, None\n","                gc.collect()\n","\n","\n","\n","                assert atten_map.device == CPU\n","                atten_map = atten_map.to(GPU)\n","                assert atten_map.device == GPU\n","\n","                #send atten_map, x, MLP layer to GPU for MLP stuff\n","\n","\n","                activation = transformer.transformer_layers[j].mlp(atten_map)\n","                atten_map = None\n","                gc.collect()\n","                #print(f\"activation_shape : {activation.shape}\")\n","                act_home[k] = activation\n","\n","                activation = None\n","                gc.collect()\n","    #             file_name = f\"a_{k}.pt\"\n","    #             torch.save(act_home, filename)\n","\n","              #check if all 3 layers are filled:\n","            if j != 0 and j % modder == remainder:\n","                step_size = opt_config[\"file_size\"]\n","                filename = f\"k_{j-step_size+1}-{j}.pt\"\n","                torch.save(k_home, filename)\n","                filename = f\"v_{j-step_size+1}-{j}.pt\"\n","                torch.save(v_home, filename)\n","            move_weights(layer_weights, GPU)\n","            #print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=5))\n","\n","    x = None\n","    gc.collect()\n","    # for activations in act_home:\n","    x = transformer.out_linear(act_home)\n","    probs = nn.Softmax()(x)\n","    #print(probs.shape)\n","    ### Kind of stuck here, dont know how to get the predicted token to add to the batches for next iter.\n","    ##### Getting output of size (8,16,32) so for every input getting a list of [32 numbers]\n","    # Get the argmax along the vocabulary dimension (-1)\n","    output_ids = torch.argmax(probs, dim=-1)\n","    probs = None\n","    #print(output_ids.shape)\n","    # # output_ids should have shape (8, 16, 32)\n","    new_batches = []\n","    for batch_idx, batch in enumerate(batches):\n","        # print(f\"device - batch : {batch.device}\")\n","        # print(f\"device - output_ids : {output_ids.device}\")\n","        batch_with_new_tokens = torch.cat((batch, output_ids[batch_idx][:,-1].unsqueeze(-1)), dim=1)\n","        new_batches.append(batch_with_new_tokens)\n","    output_ids = None\n","    batches = new_batches\n","    new_batches = None\n","    \n","    prefilling_finish_decode_start_time = time.time()\n","\n","    #### DECODE\n","    print(\"--------------------------------DECODE----------------------------------\")\n","    x_shape, k_shape, v_shape, act_shape=0,0,0,0\n","    with torch.no_grad():\n","        for i in range(1, opt_config[\"max_seq_len\"]):\n","            # print(f\"i={i}\")\n","            act_home = torch.empty(opt_config[\"num_gpu_batches\"], opt_config[\"gpu_batch_size\"], 1, opt_config[\"input_dim\"]).to(GPU)\n","            #k_new = torch.empty(opt_config[\"num_hidden_layers\"], opt_config[\"num_gpu_batches\"], opt_config[\"gpu_batch_size\"], opt_config[\"prompt_len\"] + i, opt_config[\"input_dim\"]).to(CPU)\n","            #v_new = torch.empty(opt_config[\"num_hidden_layers\"], opt_config[\"num_gpu_batches\"], opt_config[\"gpu_batch_size\"], opt_config[\"prompt_len\"] + i, opt_config[\"input_dim\"]).to(CPU)\n","            print(\"act_home init\")\n","            k_new = torch.empty(opt_config[\"file_size\"], opt_config[\"num_gpu_batches\"], opt_config[\"gpu_batch_size\"], opt_config[\"prompt_len\"] + i, opt_config[\"input_dim\"]).to(CPU)\n","            print(\"k home init\")\n","\n","            v_new = torch.empty(opt_config[\"file_size\"], opt_config[\"num_gpu_batches\"], opt_config[\"gpu_batch_size\"], opt_config[\"prompt_len\"] + i, opt_config[\"input_dim\"]).to(CPU)\n","            print(\"v home init\")\n","\n","            assert act_home.device == GPU\n","    #         assert k_home.device == CPU\n","    #         assert v_home.device == CPU\n","            # print(\"act_home, k_new, v_new initialized\")\n","\n","            loaded = False\n","\n","            for j in range(opt_config[\"num_hidden_layers\"]):\n","                layer_weights = all_weights[j]\n","                print(\"weights read\")\n","                move_weights(layer_weights, CPU)\n","                print(\"weights loaded\")\n","\n","#                 if not loaded:\n","#                       step_size = opt_config[\"file_size\"]\n","\n","#                       # Calculate the start and end indices for the filename\n","#                       start_index = j\n","#                       end_index = j + remainder\n","#                       filename = f\"k_{start_index}-{end_index}.pt\"\n","#                       k_home = torch.load(filename)\n","#                       filename = f\"v_{start_index}-{end_index}.pt\"\n","#                       v_home = torch.load(filename)\n","#                       loaded = True\n","\n","                for k in range(opt_config[\"num_gpu_batches\"]):\n","                    if j != 0:\n","                        x = act_home[k]\n","                        x_shape = x.shape\n","                        # print(f\"x shape when j={j}: {x.shape}\")\n","                    else:\n","                        # x = transformer.input_embed(batches[k])\n","                        x = batches[k][:, -1].view(-1,1)\n","                        # print(f\"x shape : {x.shape}\")\n","                        # print(f\"x  : {x}\")\n","                        x = transformer.input_embed(x)\n","                        x_shape = x.shape\n","                        # print(f\"x shape when j={j}: {x.shape}\")\n","                        # print(f\"x shape : {x.shape}\")\n","\n","                    # print(f\"-------{j}-------\")\n","                    x = x.to(CPU)\n","                    K = layer_weights[\"weight_k\"](x)\n","                    Q = layer_weights[\"weight_q\"](x)\n","                    V = layer_weights[\"weight_v\"](x)\n","                    print(f\"KQV for layer: {j} and box is{k}\")\n","                    k_shape = K.shape\n","                    v_shape = V.shape\n","                    # With file\n","                    #old_k = k_home[j % modder][k]\n","                    #old_v = v_home[j % modder][k]\n","                    \n","                    #without File\n","                    old_k = k_home[j][k]\n","                    old_v = v_home[j][k]\n","\n","                    # print(f\"this K : {K.shape}\")\n","                    K = torch.cat((old_k, K), dim=1)\n","                    V = torch.cat((old_v, V), dim=1)\n","                    print(f\"Concat done for layer: {j} and box is{k}\")\n","\n","                    k_shape = K.shape\n","                    v_shape = V.shape\n","\n","                    # print(f\"old_k : {old_k.shape} for j={j}\")\n","                    # print(f\"K : {K.shape} for j={j}\")\n","                    atten_map, _ = transformer.transformer_layers[j].mha(Q,K,V, need_weights=False)\n","                    print(f\"MHA Done for layer: {j} and box is{k}\")\n","                    #move_weightstten_map, \"cuda: 0\")\n","                    # print(f\"k shape when j={j}: {K.shape}\")\n","                    print(K.shape)\n","                    print(k_new[j % modder][k].shape)\n","                    #with File\n","                    #k_new[j % modder][k] = K\n","                    #v_new[j % modder][k] = V\n","                    \n","                    #Without File\n","                    k_new[j][k] = K\n","                    v_new[j][k] = V                    \n","                    \n","\n","                    assert atten_map.device == CPU\n","                    atten_map = atten_map.to(GPU)\n","                    assert atten_map.device == GPU\n","                    print(f\"MLP started for layer: {j} and box is{k}\")\n","\n","                    #send atten_map, x, MLP layer to GPU for MLP stuff\n","                    activation = transformer.transformer_layers[j].mlp(atten_map)\n","                    print(f\"MLP finished for layer: {j} and box is{k}\")\n","                  \n","                    act_shape = activation.shape\n","                    # print(f\"activation_shape : {activation.shape}\")\n","                    act_home[k] = activation\n","                    # print(act_home[k].shape)\n","                print(\"act home saved\")\n","#                 if j != 0 and j % modder == remainder:\n","#                   print(\"Inside if\")\n","#                   print(j, modder, remainder)\n","#                   #if j % modder == remainder:\n","#                   loaded = False\n","#                   step_size = opt_config[\"file_size\"]\n","#                   filename = f\"k_{j-step_size+1}-{j}.pt\"\n","#                   torch.save(k_new, filename)\n","#                   filename = f\"v_{j-step_size+1}-{j}.pt\"\n","#                   torch.save(v_new, filename)\n","                print(\"move weight started\")\n","                move_weights(layer_weights, GPU)\n","                print(\"move weight ended\")\n","\n","            k_home, v_home = k_new, v_new\n","\n","\n","            # for activations in act_home:\n","            print(f\"Out Linear Started\")\n","\n","            x = transformer.out_linear(act_home)\n","            print(f\"Out Linear Finished\")\n","            probs = nn.Softmax()(x)\n","            print(f\"Softmax finsihed\")\n","            #print(f\"probs : {probs.shape}\")\n","            ### Kind of stuck here, dont know how to get the predicted token to add to the batches for next iter.\n","            ##### Getting output of size (8,16,32) so for every input getting a list of [32 numbers]\n","            # Get the argmax along the vocabulary dimension (-1)\n","            output_ids = torch.argmax(probs, dim=-1)\n","            print(f\"Softmax finsihed\")\n","            #print(f\"output_ids : {output_ids.shape}\")\n","            # # output_ids should have shape (8, 16, 32)\n","            new_batches = []\n","            for batch_idx, batch in enumerate(batches):\n","                batch_with_new_tokens = torch.cat((batch, output_ids[batch_idx][:,-1].unsqueeze(-1)), dim=1)\n","                new_batches.append(batch_with_new_tokens)\n","            print(f\"batching finsihed\")\n","\n","            batches = new_batches\n","            print(f\"token {i} generated\")\n","    #print(f\"batches: {len(batches)}, {len(batches[1])}\")\n","    #print(f\"batches: {batches[0][0]}\")\n","    #print(k_home.shape)\n","        # print(batches[0][0])\n","    \n","    decode_finish_time = time.time()\n","    prefill_time = prefilling_finish_decode_start_time - prefilling_start_time\n","    decode_time = decode_finish_time - prefilling_finish_decode_start_time\n","    total_inference_time = decode_finish_time - prefilling_start_time\n","\n","    prefill_throughput = num_prompts / prefill_time\n","    decode_throughput = num_prompts * 31 / decode_time\n","    total_inference_throughput = num_prompts / total_inference_time\n","\n","    print(f\"prefill_throughput : {prefill_throughput}\")\n","    print(f\"decode_throughput : {decode_throughput}\")\n","    print(f\"total_inference_throughput : {total_inference_throughput}\")\n","\n","\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZHmYGQ-ZK9t","trusted":true},"outputs":[],"source":["cuda_memory_stats = torch.cuda.memory_stats(torch.device('cuda'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YrYPtAm3ZK9t","trusted":true},"outputs":[],"source":["print(\"----------GPU STATS-------------------------\")\n","# Display active and allocated memory usage\n","print(\"Active Memory Usage:\")\n","print(f\"Total Active Memory: {cuda_memory_stats['active_bytes.all.current'] / 1e6} MB\")\n","print(f\"Peak Active Memory: {cuda_memory_stats['active_bytes.all.peak'] / 1e6} MB\")\n","print(f\"Active Large Pool Memory: {cuda_memory_stats['active_bytes.large_pool.current'] / 1e6} MB\")\n","print(f\"Peak Active Large Pool Memory: {cuda_memory_stats['active_bytes.large_pool.peak'] / 1e6} MB\")\n","print(f\"Active Small Pool Memory: {cuda_memory_stats['active_bytes.small_pool.current'] / 1e6} MB\")\n","print(f\"Peak Active Small Pool Memory: {cuda_memory_stats['active_bytes.small_pool.peak'] / 1e6} MB\")\n","\n","print(\"\\nAllocated Memory Usage:\")\n","print(f\"Total Allocated Memory: {cuda_memory_stats['allocated_bytes.all.current'] / 1e6} MB\")\n","print(f\"Peak Allocated Memory: {cuda_memory_stats['allocated_bytes.all.peak'] / 1e6} MB\")\n","print(f\"Allocated Large Pool Memory: {cuda_memory_stats['allocated_bytes.large_pool.current'] / 1e6} MB\")\n","print(f\"Peak Allocated Large Pool Memory: {cuda_memory_stats['allocated_bytes.large_pool.peak'] / 1e6} MB\")\n","print(f\"Allocated Small Pool Memory: {cuda_memory_stats['allocated_bytes.small_pool.current'] / 1e6} MB\")\n","print(f\"Peak Allocated Small Pool Memory: {cuda_memory_stats['allocated_bytes.small_pool.peak'] / 1e6} MB\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Vfe_x-OZK9t","trusted":true},"outputs":[],"source":["# Total allocated memory (estimated)\n","total_allocated_memory = cuda_memory_stats['allocated_bytes.all.peak']\n","\n","# Currently active memory\n","active_memory = cuda_memory_stats['active_bytes.all.current']\n","\n","# Print a summary\n","print(f\"Total allocated GPU memory: {total_allocated_memory / (1024**3)} GB\")\n","print(f\"Currently active GPU memory: {active_memory / (1024**3)} GB\")\n","\n","# You can also calculate the percentage of memory being used\n","percent_used = (active_memory / total_allocated_memory) * 100\n","print(f\"Percent of GPU memory used: {percent_used:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fACr2XaZZK9t","trusted":true},"outputs":[],"source":["print(f\"Total Memory Allocated: {torch.cuda.memory_stats(torch.device('cuda'))['active_bytes.all.allocated'] / (1024 ** 3):.2f} GB\")\n","print(f\"Peak Memory Usage: {torch.cuda.memory_stats(torch.device('cuda'))['active_bytes.all.peak'] / (1024 ** 3):.2f} GB\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_oqVwrCRZK9t","trusted":true},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","\n","# # Assuming you have a list of memory usage values over time\n","# memory_usage = [\n","#     torch.cuda.memory_stats(torch.device('cuda'))['active_bytes.all.allocated'] / (1024 ** 3),\n","#     # Add more values here\n","# ]\n","\n","# plt.figure(figsize=(10, 6))\n","# plt.bar(range(len(memory_usage)), memory_usage)\n","# plt.xlabel(\"Time Step\")\n","# plt.ylabel(\"Memory Usage (GB)\")\n","# plt.title(\"Memory Usage Over Time\")\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOdnD6U2ZK9t","trusted":true},"outputs":[],"source":["print(f\"Current Memory Usage: {torch.cuda.memory_stats(torch.device('cuda'))['active_bytes.all.current'] / (1024 ** 3):.2f} GB\")\n","print(f\"Number of Allocations: {torch.cuda.memory_stats(torch.device('cuda'))['allocation.all.allocated']}\")\n","print(f\"Number of Deallocations: {torch.cuda.memory_stats(torch.device('cuda'))['allocation.all.freed']}\")\n","print(f\"Number of Out-of-Memory Errors: {torch.cuda.memory_stats(torch.device('cuda'))['num_ooms']}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0puaC37HZK9u","trusted":true},"outputs":[],"source":["torch.cuda.memory_summary(device='cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"084efcd4f2024c759dee11ecd94c69c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e4bfd2004244427969d6835aaf1da67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14d090c5d5b9426ebeba54371151619f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a51c974bef834123894086fbe9e02a32","placeholder":"​","style":"IPY_MODEL_a1dfc8865e044854a5d21f19525889e0","value":" 899k/899k [00:00&lt;00:00, 2.76MB/s]"}},"167a39eb1956451dbf95c35bf7cf06ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e3f85e1ceff4da8bc39e683b5c75b2d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f3b3815c624467898f51bfdac858746":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20e696a19a5d471ebdcc436ff8dbde34":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2698d060403345148b04bf63f193996e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6ac6f9411014114809c06433921610a","IPY_MODEL_8d9ae42aadb54690b974ec2cf6f74780","IPY_MODEL_4ea04fbfd14d4bc0805f7a0b362a9671"],"layout":"IPY_MODEL_eeb478610bc74e2f91b8461475e8200c"}},"2cb896dda4134ea4b07775a2df82e156":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"333a669d9cb545328eda001262eb042a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"345c613d7363438084d81067d9a18306":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3674a894012348909aea0d93c2efbec9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9c30ff46e2e44d8a8cbef4e6e0f1bfc","IPY_MODEL_d87f464484814cb2ae55aa64a74e2ec8","IPY_MODEL_9d095d3e7a2c44069835cfb8b859836d"],"layout":"IPY_MODEL_7afaa90e89414e2dad7ac07e911da8bf"}},"367bd0c3cf2a48fa8006b14b3c00284a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38063cc56d96464c8887aeb7c619f16a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_167a39eb1956451dbf95c35bf7cf06ea","max":221,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d134ff4e50db4521a18b66e167cefcf9","value":221}},"43be10eb97e04c98a6d99bbd3f5d2b63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ea04fbfd14d4bc0805f7a0b362a9671":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3edd7e5aa9a4a8eb4077fa801a47971","placeholder":"​","style":"IPY_MODEL_a242facd21144b599656052e55cf62f7","value":" 685/685 [00:00&lt;00:00, 38.8kB/s]"}},"53bdc05005ea4bd6ac4dd7940a75dfaa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59db37a6d3fe4900b1cad1f793dbac65":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a08c7f346ab4bd9bb18e6a9cf793728":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73f3d5ffa198476abe75e5d500e041ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89448e748c1a49e0a158b86ebd064072","IPY_MODEL_d6300a4882f64fe186aa69c822c886ee","IPY_MODEL_a98a5e8ef09d4474873962de3158087c"],"layout":"IPY_MODEL_367bd0c3cf2a48fa8006b14b3c00284a"}},"75336ac6fb9045fca144a0a1d858b9e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b550bb703a0b412ab564a42afb18fea5","IPY_MODEL_38063cc56d96464c8887aeb7c619f16a","IPY_MODEL_891232c8116a4c65ae28454cc57ddab5"],"layout":"IPY_MODEL_941a33dd456f45298a0753c9d4930cda"}},"79b250af1835464bbe12493a44423764":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7afaa90e89414e2dad7ac07e911da8bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bfb21a26bdf48cdba25b6018cfac792":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84517602469749a083990fa5fc3bcc8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"891232c8116a4c65ae28454cc57ddab5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_345c613d7363438084d81067d9a18306","placeholder":"​","style":"IPY_MODEL_1f3b3815c624467898f51bfdac858746","value":" 221/221 [00:00&lt;00:00, 14.0kB/s]"}},"89448e748c1a49e0a158b86ebd064072":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e4bfd2004244427969d6835aaf1da67","placeholder":"​","style":"IPY_MODEL_c8423a0b6d414315aa9714ea6416be53","value":"merges.txt: 100%"}},"8d9ae42aadb54690b974ec2cf6f74780":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a08c7f346ab4bd9bb18e6a9cf793728","max":685,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fcb370e58140494c801dcf517304491a","value":685}},"9135fd59b0814a1881bcf8985d3aa92b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"941a33dd456f45298a0753c9d4930cda":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9490adf32e9140a490b2fb48fa93a92e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bad0992479b4212a2e04c507ef12f30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53bdc05005ea4bd6ac4dd7940a75dfaa","placeholder":"​","style":"IPY_MODEL_7bfb21a26bdf48cdba25b6018cfac792","value":"vocab.json: 100%"}},"9d095d3e7a2c44069835cfb8b859836d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e18b86c903d541788ec2d7b3c0f91225","placeholder":"​","style":"IPY_MODEL_9135fd59b0814a1881bcf8985d3aa92b","value":" 651/651 [00:00&lt;00:00, 51.0kB/s]"}},"a1dfc8865e044854a5d21f19525889e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a242facd21144b599656052e55cf62f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a51c974bef834123894086fbe9e02a32":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a98a5e8ef09d4474873962de3158087c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_333a669d9cb545328eda001262eb042a","placeholder":"​","style":"IPY_MODEL_fad66a89c41e48bb80262cc3f002dc2b","value":" 456k/456k [00:00&lt;00:00, 30.1MB/s]"}},"b550bb703a0b412ab564a42afb18fea5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79b250af1835464bbe12493a44423764","placeholder":"​","style":"IPY_MODEL_43be10eb97e04c98a6d99bbd3f5d2b63","value":"special_tokens_map.json: 100%"}},"b5f90985c5f3427a83b32635d32c56d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c53ae763042a4b10b835cd21274f1589":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9bad0992479b4212a2e04c507ef12f30","IPY_MODEL_f1e7479089f846cf81c0f691ef6d1b9d","IPY_MODEL_14d090c5d5b9426ebeba54371151619f"],"layout":"IPY_MODEL_1e3f85e1ceff4da8bc39e683b5c75b2d"}},"c8423a0b6d414315aa9714ea6416be53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d134ff4e50db4521a18b66e167cefcf9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d3edd7e5aa9a4a8eb4077fa801a47971":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d52880be825c46fea09ad8e1ee35f694":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6300a4882f64fe186aa69c822c886ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d52880be825c46fea09ad8e1ee35f694","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84517602469749a083990fa5fc3bcc8d","value":456318}},"d87f464484814cb2ae55aa64a74e2ec8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_59db37a6d3fe4900b1cad1f793dbac65","max":651,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b5f90985c5f3427a83b32635d32c56d5","value":651}},"d9c30ff46e2e44d8a8cbef4e6e0f1bfc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9490adf32e9140a490b2fb48fa93a92e","placeholder":"​","style":"IPY_MODEL_084efcd4f2024c759dee11ecd94c69c0","value":"config.json: 100%"}},"dfdbe51a6f0e455b91c3c194dd15f064":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e18b86c903d541788ec2d7b3c0f91225":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeb478610bc74e2f91b8461475e8200c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f11ffeb0db6d46d0a15f361d66864ef7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1e7479089f846cf81c0f691ef6d1b9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfdbe51a6f0e455b91c3c194dd15f064","max":898822,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2cb896dda4134ea4b07775a2df82e156","value":898822}},"f6ac6f9411014114809c06433921610a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f11ffeb0db6d46d0a15f361d66864ef7","placeholder":"​","style":"IPY_MODEL_20e696a19a5d471ebdcc436ff8dbde34","value":"tokenizer_config.json: 100%"}},"fad66a89c41e48bb80262cc3f002dc2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcb370e58140494c801dcf517304491a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
